{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNFLWpeIA2+TehfoiVt1qUJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sk-226/comparison-of-text-similarity/blob/main/comparsion_of_text_similarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GDSC Summer Hackathon Team-2"
      ],
      "metadata": {
        "id": "mnqkiZtIUFH_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zbH030aKtst",
        "outputId": "4f8824ad-3c03-42fa-9e70-274ac41d81d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.0+cu121)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: janome in /usr/local/lib/python3.10/dist-packages (0.5.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scikit-learn nltk transformers torch sentence-transformers janome"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "qqvEMX2MK1vw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 英語"
      ],
      "metadata": {
        "id": "g2x8YfZkNtXF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# nltkの初期設定\n",
        "nltk.download('punkt')  # トークン化用データセットをダウンロード\n",
        "nltk.download('stopwords')  # 停止語のデータセットをダウンロード\n",
        "stop_words = set(stopwords.words('english'))  # 英語の停止語を取得"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnBRUExFLMOX",
        "outputId": "948b042a-34e8-4f41-bb2c-f5ad058ebd69"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 題名の正解と回答を変数に格納\n",
        "correct_title = \"A beautiful sunset over the mountains\"\n",
        "proposed_title = \"A gorgeous sunset in the hills\""
      ],
      "metadata": {
        "id": "nDfdb8JuLOlN"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Cosine Similarity\n",
        "def cosine_similarity_example(correct, proposed):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform([correct, proposed])\n",
        "    cosine_sim = cosine_similarity(vectors[0:1], vectors[1:2])\n",
        "    return cosine_sim[0][0]\n",
        "\n",
        "# 2. Jaccard Similarity\n",
        "def jaccard_similarity(correct, proposed):\n",
        "    correct_tokens = set(word_tokenize(correct.lower()))\n",
        "    proposed_tokens = set(word_tokenize(proposed.lower()))\n",
        "\n",
        "    correct_tokens = correct_tokens.difference(stop_words)\n",
        "    proposed_tokens = proposed_tokens.difference(stop_words)\n",
        "\n",
        "    intersection = correct_tokens.intersection(proposed_tokens)\n",
        "    union = correct_tokens.union(proposed_tokens)\n",
        "\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "# 3. Word Embedding (using Sentence-BERT)\n",
        "def word_embedding_similarity(correct, proposed):\n",
        "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "    correct_emb = model.encode(correct, convert_to_tensor=True)\n",
        "    proposed_emb = model.encode(proposed, convert_to_tensor=True)\n",
        "\n",
        "    cosine_sim = torch.nn.functional.cosine_similarity(correct_emb, proposed_emb, dim=0)\n",
        "    return cosine_sim.item()\n",
        "\n",
        "# 4. BERT (using HuggingFace transformers)\n",
        "def bert_similarity(correct, proposed):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    inputs_correct = tokenizer(correct, return_tensors='pt', truncation=True, padding=True)\n",
        "    inputs_proposed = tokenizer(proposed, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct_emb = model(**inputs_correct).last_hidden_state.mean(dim=1)\n",
        "        proposed_emb = model(**inputs_proposed).last_hidden_state.mean(dim=1)\n",
        "\n",
        "    cosine_sim = torch.nn.functional.cosine_similarity(correct_emb, proposed_emb)\n",
        "    return cosine_sim.item()\n",
        "\n",
        "\n",
        "def measure_time(func, *args):\n",
        "    start_time = time.time()\n",
        "    result = func(*args)\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    return result, elapsed_time"
      ],
      "metadata": {
        "id": "dFaXo4sPLQ2P"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 実行\n",
        "cosine_sim, cosine_time = measure_time(cosine_similarity_example, correct_title, proposed_title)\n",
        "jaccard_sim, jaccard_time = measure_time(jaccard_similarity, correct_title, proposed_title)\n",
        "word_emb_sim, word_emb_time = measure_time(word_embedding_similarity, correct_title, proposed_title)\n",
        "bert_sim, bert_time = measure_time(bert_similarity, correct_title, proposed_title)"
      ],
      "metadata": {
        "id": "DqSBMusqLTh4"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 結果を出力\n",
        "print(f\"Cosine Similarity: {cosine_sim:.4f} (Time: {cosine_time:.4f} seconds)\")\n",
        "print(f\"Jaccard Similarity: {jaccard_sim:.4f} (Time: {jaccard_time:.4f} seconds)\")\n",
        "print(f\"Word Embedding Similarity: {word_emb_sim:.4f} (Time: {word_emb_time:.4f} seconds)\")\n",
        "print(f\"BERT Similarity: {bert_sim:.4f} (Time: {bert_time:.4f} seconds)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49yuDDTKLZ9K",
        "outputId": "9104b16b-b325-4dd4-fd48-e7401cedcbbc"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.2523 (Time: 0.0125 seconds)\n",
            "Jaccard Similarity: 0.2000 (Time: 0.0009 seconds)\n",
            "Word Embedding Similarity: 0.9188 (Time: 1.1134 seconds)\n",
            "BERT Similarity: 0.9445 (Time: 1.7191 seconds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 日本語"
      ],
      "metadata": {
        "id": "C1F0UpQJNwnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import numpy as np\n",
        "from janome.tokenizer import Tokenizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel"
      ],
      "metadata": {
        "id": "UY_PsR2cLzif"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Janomeの形態素解析器を初期化\n",
        "tokenizer = Tokenizer()"
      ],
      "metadata": {
        "id": "yEYdyU4cN2_p"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語テキストの正解と回答を変数に格納\n",
        "correct_title = \"美しい夕日が山々の上に広がる\"\n",
        "proposed_title = \"壮大な夕焼けが丘に映る\""
      ],
      "metadata": {
        "id": "54-KPAeTN4VC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語テキストを形態素解析して単語に分割\n",
        "def tokenize(text):\n",
        "    return \" \".join([token.surface for token in tokenizer.tokenize(text)])\n",
        "\n",
        "# 1. Cosine Similarity\n",
        "def cosine_similarity_example(correct, proposed):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    vectors = vectorizer.fit_transform([correct, proposed])\n",
        "    cosine_sim = cosine_similarity(vectors[0:1], vectors[1:2])\n",
        "    return cosine_sim[0][0]\n",
        "\n",
        "# 2. Jaccard Similarity\n",
        "def jaccard_similarity(correct, proposed):\n",
        "    correct_tokens = set(tokenize(correct).split())\n",
        "    proposed_tokens = set(tokenize(proposed).split())\n",
        "\n",
        "    intersection = correct_tokens.intersection(proposed_tokens)\n",
        "    union = correct_tokens.union(proposed_tokens)\n",
        "\n",
        "    return len(intersection) / len(union)\n",
        "\n",
        "# 3. Word Embedding (using Sentence-BERT)\n",
        "def word_embedding_similarity(correct, proposed):\n",
        "    model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "    correct_emb = model.encode(correct, convert_to_tensor=True)\n",
        "    proposed_emb = model.encode(proposed, convert_to_tensor=True)\n",
        "\n",
        "    cosine_sim = torch.nn.functional.cosine_similarity(correct_emb, proposed_emb, dim=0)\n",
        "    return cosine_sim.item()\n",
        "\n",
        "# 4. BERT (using HuggingFace transformers, 日本語BERTモデルを使用)\n",
        "def bert_similarity(correct, proposed):\n",
        "    tokenizer = BertTokenizer.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "    model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese')\n",
        "\n",
        "    inputs_correct = tokenizer(correct, return_tensors='pt', truncation=True, padding=True)\n",
        "    inputs_proposed = tokenizer(proposed, return_tensors='pt', truncation=True, padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        correct_emb = model(**inputs_correct).last_hidden_state.mean(dim=1)\n",
        "        proposed_emb = model(**inputs_proposed).last_hidden_state.mean(dim=1)\n",
        "\n",
        "    cosine_sim = torch.nn.functional.cosine_similarity(correct_emb, proposed_emb)\n",
        "    return cosine_sim.item()\n",
        "\n",
        "# 時間を計測する部分\n",
        "def measure_time(func, *args):\n",
        "    start_time = time.time()\n",
        "    result = func(*args)\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    return result, elapsed_time"
      ],
      "metadata": {
        "id": "zWm3hipyN7SL"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 日本語テキストを形態素解析してトークン化\n",
        "correct_title_tokenized = tokenize(correct_title)\n",
        "proposed_title_tokenized = tokenize(proposed_title)\n",
        "\n",
        "# 実行\n",
        "cosine_sim, cosine_time = measure_time(cosine_similarity_example, correct_title_tokenized, proposed_title_tokenized)\n",
        "jaccard_sim, jaccard_time = measure_time(jaccard_similarity, correct_title_tokenized, proposed_title_tokenized)\n",
        "word_emb_sim, word_emb_time = measure_time(word_embedding_similarity, correct_title, proposed_title)\n",
        "bert_sim, bert_time = measure_time(bert_similarity, correct_title, proposed_title)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsLcSEszN-76",
        "outputId": "68ca2a38-088f-4edf-b4c6-e22a0b6e93ea"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertJapaneseTokenizer'. \n",
            "The class this function is called from is 'BertTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 結果を出力\n",
        "print(f\"Cosine Similarity: {cosine_sim:.4f} (Time: {cosine_time:.4f} seconds)\")\n",
        "print(f\"Jaccard Similarity: {jaccard_sim:.4f} (Time: {jaccard_time:.4f} seconds)\")\n",
        "print(f\"Word Embedding Similarity: {word_emb_sim:.4f} (Time: {word_emb_time:.4f} seconds)\")\n",
        "print(f\"BERT Similarity: {bert_sim:.4f} (Time: {bert_time:.4f} seconds)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dpq134WlOD5Q",
        "outputId": "26eef955-8ea3-4c9b-b71d-76e29bad5823"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.0000 (Time: 0.0112 seconds)\n",
            "Jaccard Similarity: 0.1538 (Time: 0.0036 seconds)\n",
            "Word Embedding Similarity: 0.8893 (Time: 1.1631 seconds)\n",
            "BERT Similarity: 0.9198 (Time: 3.7172 seconds)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VnWQAOJcOJHk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}